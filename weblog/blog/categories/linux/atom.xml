<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | My Delusional Dream]]></title>
  <link href="http://patrick.wagstrom.net/weblog/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://patrick.wagstrom.net/weblog/"/>
  <updated>2012-05-12T20:51:14-04:00</updated>
  <id>http://patrick.wagstrom.net/weblog/</id>
  <author>
    <name><![CDATA[Patrick Wagstrom]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Ohio LinuxFest 2009]]></title>
    <link href="http://patrick.wagstrom.net/weblog/2009/10/01/ohio-linuxfest-2009/"/>
    <updated>2009-10-01T00:51:42-04:00</updated>
    <id>http://patrick.wagstrom.net/weblog/2009/10/01/ohio-linuxfest-2009</id>
    <content type="html"><![CDATA[<p>This past weekend I left New York City and traveled to Columbus, OH for <a href="http://www.ohiolinux.org/">Ohio LinuxFest</a> 2009 (OLF). Unlike many shows, such as LinuxCon and the now defunct OpenSourceWorld, OLF is entirely community run.  That means you’ll notice a couple of different things: it’s much cheaper, there’s a wider range of attendees, and while many top name speakers attend, you’ll also get a smattering of other folks making their debuts on the conference circuit.  Such was the case for me.  OLF is where I presented the talk “Be A Wonk” that discussed how policy and law get made and what we can do as geeks to influence these issues.</p>

<p>As promised, I’ve posted <a href="http://academic.patrick.wagstrom.net/presentation-files/non-academic/2009-09-26-OhioLinuxFest.odp?attredirects=0">my slides as the original OpenOffice.org format</a> and also have posted <a href="http://academic.patrick.wagstrom.net/presentation-files/non-academic/2009-09-26-OhioLinuxFest.pdf?attredirects=0">a PDF version of my slides</a>, which is decidedly less sexy.  To make things even easier, <a href="http://www.slideshare.net/pridkett/be-a-wonk">here’s a copy of the slides on SlideShare</a>.  At some point I might record some dialog to go along with the slides, but for now this is what you get.</p>

<p>In previous years I may have been a bit too critical about some of the talks at OLF, there would usually be a few periods during the day that I couldn’t find anything good to see.  I’m pleased to say that was never the case this year.  Among an awesome schedule of talks there were standout talks from <a href="http://www.silwenae.org/blog/">Paul Cutler</a> about GNOME 3.0 (GNOME Shell could be some new hotness), <a href="http://stompbox.typepad.com/">Jorge Castro</a> about how to fail at building a project, <a href="http://spot.livejournal.com/">Tom Calloway</a> about licenses for projects, and <a href="http://ubuntulinuxtipstricks.blogspot.com/">Mackenzie Morgan</a> about how to handle translating between package management.</p>

<p>The keynotes were both excellent.  <a href="http://www.brainofshawn.com/">Shawn Powers</a> did a great job of setting up the conference in the morning, despite having lost his slides on the flight in. Although there was some Microsoft bashing in his talk, there was also plenty of realistic Linux bashing, the subject of which will be a future post.  He addressed some of the failures of Linux and people selling Linux machines (GMA 500 anyone) and also encouraged us to be honest about the faults of Linux, rather than just glossing over them.  The conference closed with <a href="http://www.cs.dartmouth.edu/%7Edoug/">Doug McIlroy</a> — the man who invented Unix pipes — describing some of the problems with sophisticated software and how often we just need to approach the problem from a different aspect rather than adding in extra complexity.</p>

<p>I have to say that OLF 2009 was a smashing success and much better than previous years.  This was my fourth time attending and the conference has certainly evolved — the team that plans and runs the conference it is much more polished and organized than in years past and we’re starting to get some great PR from the conference.  There’s a solid pool of commercial sponsors along with numerous community based projects that return year after year. It’s good to see an all volunteer event thriving.</p>

<p>Going forward, there are good things in store for OLF.  OLF has already put itself in a position as one of the most gender and ethnic friendly conferences in Open Source, thanks to their commitment to get women and other minorities represented.  I’m pleased that many of the key organizers were women as were some of the great speakers.  After the main conference they also held a workshop on diversity in Open Source — which addressed many topics regarding participation in OSS, one of which is why only 2% of OSS developers are female, but nearly 50% of professional software developers are female.</p>

<p>If you can make it to Columbus on the last weekend of September 2010, I highly recommend attending OLF 2010, as I’m certain it will be another great community organized conference.</p>

<div style="width:425px;text-align:center" id="__ss_2101534"><div><a style="font:14px Helvetica,Arial,Sans-serif;display:block;margin:12px 0 3px 0;text-decoration:underline;" href="http://www.slideshare.net/pridkett/be-a-wonk" title="Be A Wonk">Be A Wonk - A Presentation by Patrick Wagstrom at OLF 2009</a></div><object style="margin:0px" width="425" height="355"><param name="movie" value="http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=olf2009-090930231128-phpapp01&rel=0&stripped_title=be-a-wonk" /><param name="allowFullScreen" value="true"/><param name="allowScriptAccess" value="always"/><embed src="http://static.slidesharecdn.com/swf/ssplayer2.swf?doc=olf2009-090930231128-phpapp01&rel=0&stripped_title=be-a-wonk" type="application/x-shockwave-flash" allowscriptaccess="always" allowfullscreen="true" width="425" height="355"></embed></object><div style="font-size:11px;font-family:tahoma,arial;height:26px;padding-top:2px;">View more <a style="text-decoration:underline;" href="http://www.slideshare.net/">presentations</a> from <a style="text-decoration:underline;" href="http://www.slideshare.net/pridkett">pridkett</a>.</div></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simple Full Disk Image Backups With dd]]></title>
    <link href="http://patrick.wagstrom.net/weblog/2009/05/08/simple-full-disk-image-backups-with-dd/"/>
    <updated>2009-05-08T13:12:41-04:00</updated>
    <id>http://patrick.wagstrom.net/weblog/2009/05/08/simple-full-disk-image-backups-with-dd</id>
    <content type="html"><![CDATA[<p>After obtaining a new laptop one of the first things I always do is to make an image of the primary hard drive.  I then copy this image to another computer with a lot of hard disk space and leave it there as a backup should something ever go really wrong with the laptop.  There are a variety of tools both commercial and Open Source that make this process relatively easy.  The most well known tools are probably <a href="http://www.symantec.com/norton/ghost">Norton Ghost</a> and <a href="http://www.acronis.com/">Acronis TrueImage</a>.  In the Open Source world there are some decent alternatives such as <a href="http://www.fogproject.org/">FOG</a> and <a href="http://www.partimage.org/">PartImage</a>.  Most of these tools have varying levels of intelligence that allow them to copy only the bits of the filesystem that are in use, dramatically reducing the amount of space that it takes to create an image.</p>

<p>However, all these solutions require me to actually obtain the software and create images will only work with that specific software. The not only results in me needed to download additional software, but also needing to hope that the software continues to be maintained in the future.  As I don't want to be locked into a single piece of software what may not be maintained in the future, I typically do my backups with a much more simple tool, the standard unix tool dd.</p>

<p>The manual page for dd says that it "cop[ies] and converts a file".  In this case, I take advantage of the fact that all block devices present themselves as files under Linux and simply make a copy of the hard drive using that method.  If I need to restore the image in the future, I just write the image back to the hard drive using dd.</p>

<p>Because more and more computers are shipping without CD/DVD drives, especially in the category of netbooks and ultraportables we can't just use a Linux live CD.  In my case, I've used this method to back up an IBM Thinkpad x31 and a Lenovo Thinkpad x61 tablet, neither of which have optical drives.  The first step is to create a bootable USB stick with a Linux image on the disk.  You'll use this to boot into Linux and run dd -- after all, you can't image a drive if you're currently using it.  There are various tools to do this, but I've had really great luck with <a href="http://unetbootin.sourceforge.net/">UNetbootin</a>.  If you want to use the very simple method, you can just download the software, and it will take care of automatically downloading the CD image and coping it to the USB stick to make it bootable.  It also provides an option to take an ISO image already on your hard drive and make it bootable on the USB stick.  Because it has excellent hardware support and the install CD is also a live CD, I usually use the latest version of <a href="http://www.ubuntu.com/">Ubuntu</a>.</p>

<p>With the USB stick in hand, simply boot the computer you wish to clone from the USB drive.  Once the Linux desktop appears open up a terminal and use sudo to become root (note: some Linux images have you running root all the time).  Now you're going to use the dd command to make an image of the primary hard disk, which in newer machines is /dev/sda (some older machines may have it as /dev/hda).  Except that we need someplace to store the image, most of the time if you've put a Linux CD image on a USB drive it will not be writable, so unless you've got another large USB storage device, you'll need to copy it across the network to another machine.  Here's a diagram of what we're going to do:</p>

<p><div class='bogus-wrapper'><notextile>	<figure class='center'>
		<img src="/media/2009/05/dddriveimage.png">
		<figcaption>Drive Imaging Process </figcaption>
	</figure></notextile></div></p>

<p>Orange is operations that will happen on the local computer, and blue is operations that will happen on the remote computer where the image will be stored.  Files generated through this process can be pretty large, so doing this over a wired network is going to work better.</p>

<p>In the first step, we use dd to read the data from the hard disk, this is then piped via a standard unix pipe to bzip2, which will compress the data.  Alternatively at this point you can use gzip if you're worried about the CPU overhead.  This is a long process that I usually let run for a couple of hours.  From there the output of bzip2 is sent to SSH, which connects to SSH on the remote machine.  Then, as we're still dealing with standard output, we can use cat to redirect the output to a file.  This entire operation can be done with a single command and without the need to create any intermediate files.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>dd <span class="k">if</span><span class="o">=</span>/dev/sda <span class="nv">of</span><span class="o">=</span>/dev/stdout <span class="nv">bs</span><span class="o">=</span>1M | bzip2 | ssh USERNAME@remotehost <span class="s2">&quot;cat - &gt; drive.img.bz2&quot;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>There are a few parameters which may need a bit of explanation.  With regards to dd, the if and of arguments specify the input and output file, here we use /dev/stdout to indicate that we want dd to send the output to standard output.  I also set the blocksize to 1 megabyte with the bs argument.  This is then sent to bzip2 which will compress the standard input stream and send it to standard output.  SSH then takes this information, logs onto remote computer "remotehost" as user "USERNAME", and then uses cat to save the image to a file.  This process will not show any sort of progress as you do it.  You can get a rough idea of how much data has been processed by looking at the size of drive.img.bz2 on the remote machine.  In my experience images created this way tend to be about 1/10th the size of the drive.  Alternatively, you can work the <a href="http://www.ivarch.com/programs/pv.shtml">pv</a> command into your commands to get a better estimate of progress.  To restore the drive image to the drive, you can use the following command:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>ssh USERNAME@remotehost <span class="s2">&quot;cat drive.img.bz2&quot;</span> | bzip2 -dc | dd <span class="k">if</span><span class="o">=</span>/dev/stdin <span class="nv">of</span><span class="o">=</span>/dev/sda <span class="nv">bs</span><span class="o">=</span>1M
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>In most cases you can even use this method if you upgrade to a different disk, but you'll need to run something like <a href="http://gparted.sourceforge.net/">gparted</a> or <a href="http://www.symantec.com/norton/partitionmagic">Partition Magic</a> to update the size of the filesystem and do some small repairs on the drive after restoring the image.</p>

<p>This method doesn't generate the smallest files possible -- notably if you're working with a disk that has already been used many times, it will not generate good compression because many of the old bits are still floating around on the disk.  More advanced solutions take into account the structure of the filesystem and just code that large segments are supposed to have no data, which can provide substantial savings.  However, for a method that is quick and works with almost every system out there, I've found that this works wonders, and I know that dd isn't going anywhere.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Taking "nice" Processes Into Account With Powernowd]]></title>
    <link href="http://patrick.wagstrom.net/weblog/2009/02/04/taking-nice-processes-into-account-with-powernowd/"/>
    <updated>2009-02-04T12:31:00-05:00</updated>
    <id>http://patrick.wagstrom.net/weblog/2009/02/04/taking-nice-processes-into-account-with-powernowd</id>
    <content type="html"><![CDATA[<p>Almost every modern CPU supports the ability to dynamically change its clock frequency -- typically done to save power when there are lesser requirements placed on the system.  While the benefits for a laptop are obvious, more battery life and less heat, they may be a bit harder to understand for a desktop machine.  One application, however, where you should be particularly aware of CPU frequency scaling is with media center boxes.  The less often the CPU needs to speed up, the less heat, the quieter the fan, and the lower your power bill.  On my system, the differences can be pretty dramatic, with the CPU taking an extra 20-30 watts when running at full speed versus slower rates.</p>

<p>Within Linux, there are numerous ways that the CPU frequency is controlled, for example with <a href="http://deater.net/john/powernowd.html">powernowd</a> or the kernel level <a href="http://www.linuxinsight.com/ols2006_the_ondemand_governor.html">ondemand governor</a>, which is the default with most Linux distributions.  One of the aspects of these technologies is that when the system is heavily loaded with processes with high "nice" levels, the CPU will not speed up.  For most cases this is fine, but when running a MythTV box, it may be desirable to allow nice processes to increase the CPU speed -- particularly as these nice processes may be things like transcoding or commercial cutting jobs.  Allowing the CPU to ramp up is particular helpful for evenings where many programs are recorded.  On a Monday night where my MythTV box grabs five HD programs, without allowing the CPU to speed up, it can take over a day to commercial flag and trascode everything.  When the CPU can speed up this time drops to about 8 hours.</p>

<p>Under Ubuntu the default configuration is to use the "ondemand" governor, which means there is no daemon running and the CPU will scale with demand and load.  However, there is nasty line buried in the /etc/init.d/powernowd startup script:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'> <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;pre&gt;&lt;code&gt;use_ondemand<span class="o">()</span> <span class="o">{</span>
</span><span class='line'>    <span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;$OPTIONS&quot;</span> !<span class="o">=</span> <span class="s2">&quot;-q&quot;</span> <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'><span class="k">        return </span>1
</span><span class='line'>    <span class="k">fi</span>
</span><span class='line'><span class="k">    </span><span class="nv">status</span><span class="o">=</span>1  <span class="c"># return error, if no cpu dirs are found</span>
</span><span class='line'>    <span class="k">for </span>x in /sys/devices/system/cpu/cpu<span class="o">[</span>0-9<span class="o">]</span>*/; <span class="k">do</span>
</span><span class='line'><span class="k">        if</span> <span class="o">[</span> ! -d <span class="nv">$x</span> <span class="o">]</span> <span class="o">||</span> <span class="o">[</span> ! -f <span class="nv">$x</span><span class="s2">&quot;cpufreq/scaling_governor&quot;</span> <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'><span class="k">            continue</span>
</span><span class='line'><span class="k">        fi</span>
</span><span class='line'><span class="k">        </span><span class="nb">echo</span> -n ondemand &amp;gt; <span class="nv">$x</span><span class="s2">&quot;cpufreq/scaling_governor&quot;</span>
</span><span class='line'>        <span class="nv">status</span><span class="o">=</span><span class="nv">$?</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">[</span> <span class="nv">$status</span> !<span class="o">=</span> 0 <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'><span class="k">        return</span> <span class="nv">$status</span>
</span><span class='line'>        <span class="k">fi</span>
</span><span class='line'>        <span class="c"># The default behaviour of powernowd is to ignore nice load:</span>
</span><span class='line'>        <span class="k">if</span> <span class="o">[</span> -f <span class="nv">$x</span><span class="s2">&quot;cpufreq/ondemand/ignore_nice_load&quot;</span> <span class="o">]</span>; <span class="k">then</span>
</span><span class='line'><span class="k">            </span><span class="nb">echo</span> -n 1 &amp;gt; <span class="nv">$x</span><span class="s2">&quot;cpufreq/ondemand/ignore_nice_load&quot;</span>
</span><span class='line'>        <span class="k">fi</span>
</span><span class='line'><span class="k">    done</span>
</span><span class='line'><span class="k">    return</span> <span class="nv">$status</span>
</span><span class='line'><span class="o">}</span>
</span><span class='line'>&lt;/code&gt;&lt;/pre&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Line 108 is the culprit of our problems.  By echoing the string <code>1</code> to <code>/sys/devices/system/cpu/cpu[0-9]*/cpufreq/ondemand/ignore_nice_load</code>, the ondemand governor will not take into account nice loads.  Under previous versions of <a href="http://www.ubuntu.com/">Ubuntu</a>, or when you're not using ondemand and actually need powernowd to run, this change was easily done <a href="/weblog/2006/01/13/faster-transcoding/">altering some settings in /etc/defaults</a> as I detailed 3 years ago.  However, now you need to hack the script yourself.  The simplest way is just to change the <code>1</code> to a <code>0</code> on line 102 of <code>/etc/init.d/powernowd</code>, giving the following line:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>            echo -n 0 &gt; $x"cpufreq/ondemand/ignore_nice_load"
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Change this setting, then you can either reboot, or run <code>/etc/init.d/powernowd restart</code> as root.  If you watch your CPU speed (check out /proc/cpuinfo), you should notice that nice processes now speed up the CPU.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Simple Script to Clean Up Your MP3 Directory]]></title>
    <link href="http://patrick.wagstrom.net/weblog/2009/01/21/simple-script-to-clean-up-your-mp3-directory/"/>
    <updated>2009-01-21T15:42:16-05:00</updated>
    <id>http://patrick.wagstrom.net/weblog/2009/01/21/simple-script-to-clean-up-your-mp3-directory</id>
    <content type="html"><![CDATA[<p>Since getting an iPhone I've put a bit more care into making sure that all of my music is ripped and available.  My music resides on a <a href="http://www.drobo.com/">Drobo</a> plugged into my Linux box and shared via Samba to Windows and iTunes.  I've chosen to have iTunes organize my music folder automatically.  One down side of this is that iTunes then names files slightly differently and moves files around.  This resulted in hundreds of directories laying around with no content.  It also made it a pain when I wanted to find the actual file outside of iTunes -- was "Everybody Hurts" in REM, R.E.M, or R.E.M_?  Because iTunes has organized my music only one of those directories has any content in it, I want to nuke the other ones.</p>

<p>I set about to create a script that will look through a directory tree and find all the directories that do not contain any music at lower levels and remove those directories.  Many of the directories have other crufty files in there, but I'm pretty certain there shouldn't be anything other than music and covers in my music repository.  However, this means that I can't just delete everything that isn't a music file, because I want to keep covers in directories that still have music.</p>

<p>I accomplished this via a two stage script that first finds all subdirectories.  Then for each subdirectory find is executed again to get a count of the number of music files in the path.  If there are no music files in the directory, then the directory gets nuked.  Here's the script:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>&lt;/p>
</span><span class='line'>
</span><span class='line'>&lt;pre>&lt;code>#!/bin/bash
</span><span class='line'>
</span><span class='line'>IFS=$'\n'
</span><span class='line'>for td in $(find . -type d); do
</span><span class='line'>    FC=$(find "$td" -type f -iname "*.mp3" -or -iname "*.aac" -or -iname "*.m4?"| wc -l)
</span><span class='line'>    if [ $FC == "0" ]; then
</span><span class='line'>        echo "*** $td $FC"
</span><span class='line'>        # uncomment these two lines if you'd like to be prompted to hit return to nuke stuff
</span><span class='line'>        # ls -lR $td
</span><span class='line'>        # read x
</span><span class='line'>        rm -rf "$td"
</span><span class='line'>    fi
</span><span class='line'>done
</span><span class='line'>&lt;/code>&lt;/pre>
</span><span class='line'>
</span><span class='line'>&lt;p></span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>There's a couple of commented lines in the middle there that you can uncomment and it will show a directory listing before waiting for you to hit return and nuke the directory.  As near as I can tell this worked perfectly for me, of course, it could easily eat your music files too.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu Intrepid Ibex is Five Kinds of Fail]]></title>
    <link href="http://patrick.wagstrom.net/weblog/2008/11/16/intrepid-is-five-kinds-of-fail/"/>
    <updated>2008-11-16T08:31:00-05:00</updated>
    <id>http://patrick.wagstrom.net/weblog/2008/11/16/intrepid-is-five-kinds-of-fail</id>
    <content type="html"><![CDATA[<p>I've been a Ubuntu user since before it was called Ubuntu, back in the days when the domain name was no-name-yet.com.  It wasn't a hard switch from Fedora Core, which at the time was Core 1 or Core 2.  Ubuntu was like entering a magical world where stuff just worked.  At the time I had an IBM ThinkPad A31 -- it worked perfectly, wifi and all.  Now, things weren't quite as good as they are now, the magic of <a href="http://projects.gnome.org/NetworkManager/">NetworkManager</a> had not yet arrived, and <a href="http://dbus.freedesktop.org/">DBUS</a> was just a thought in <a href="http://www.j5live.com/">j5's mind</a>.</p>

<p>The various revisions of Ubuntu made it better and better.  Improvements to <a href="/weblog/linux/gnome-monitor-resolution.xml">multi-monitor support</a> were wonderful -- suddenly presentations were easy.  With Ubuntu Hardy basically everything on my laptop worked.  Wifi, suspend, sound, bluetooth.  It was really nice.  Unfortunately, it appears that Intrepid is a step backward -- numerous features that were supported in Hardy no longer function properly.  So, here I present my Intrepid Ibex Five Kinds of Fail.</p>

<ol>
<li><p>NetworkManager periodically disables networking (<a href="https://bugs.launchpad.net/ubuntu/intrepid/+source/network-manager/+bug/291062">291062</a>)</p></li>
<li><p>Atheros drivers are no longer properly supported (<a href="https://bugs.launchpad.net/ubuntu/intrepid/+source/linux/+bug/259157">259157</a>)</p></li>
<li><p>PulseAudio sometimes hangs after suspend/resume (<a href="https://bugs.launchpad.net/ubuntu/+source/pulseaudio/+bug/292129">292129</a>)</p></li>
<li><p>My thinkpad occasionally fails to suspend (<a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/298683">298683</a>)</p></li>
<li><p>NetworkManager forgets my WPA saved key (<a href="https://bugs.launchpad.net/ubuntu/+source/network-manager/+bug/276578">276578</a>)</p></li>
</ol>


<p>I'm not intending this to be another "Linux Sux0rs" post or anything like that, but here are five major issues that worked just fine in the last version of Ubuntu and now are total fail making the Ubuntu experience of today remind me of the Ubuntu experience from Dapper days.  There is certainly some irony that several of the features relate to wifi which Ubuntu brags about having great support for in Intrepid.  Maybe I should have heeded <a href="http://www.dnalounge.com/backstage/log/2005/10.html">jwz's advice when he lamented about upgrading three years ago</a>.  Sigh...Here's hoping Jaunty won't have the same problems.</p>
]]></content>
  </entry>
  
</feed>
